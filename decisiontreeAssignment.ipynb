{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "   - A Decision Tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. It models decisions in a tree-like structure, where each internal node represents a 'test' on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (in classification) or a numerical value (in regression).\n",
        "\n",
        "   - In the context of classification, a Decision Tree works by:\n",
        "\n",
        "   1. Splitting the Data: It starts with a root node that contains the entire dataset. The algorithm then looks for the best attribute to split the data into subsets. The 'best' attribute is typically chosen based on metrics like Gini impurity or information gain, which aim to create the most 'pure' subsets possible .\n",
        "\n",
        "   2. Recursive Partitioning: This splitting process is then recursively applied to each of the subsets created in the previous step. This continues until a stopping criterion is met.\n",
        "   \n",
        "   3. Leaf Nodes: Once the splitting stops, the final nodes are called leaf nodes. Each leaf node is assigned a class label, which is typically the majority class of the samples that end up in that node.\n",
        "\n",
        "   - How it makes predictions (classification):\n",
        "\n",
        "      When a new, unseen data point comes in, it traverses the tree from the root node down to a leaf node. At each internal node, it follows the branch corresponding to the outcome of the test on that data point's attribute. Once it reaches a leaf node, the class label assigned to that leaf node is the prediction for the new data point."
      ],
      "metadata": {
        "id": "ltI3meTuVodw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. : Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "   - Gini Impurity:It tells us how often a randomly chosen data point from the node would be incorrectly classified if we labeled it based on the class proportions.\n",
        "  Formula: Gini = 1 - (p1² + p2² + … + pk²).\n",
        "  If a node is pure (only one class), Gini is 0. Higher Gini means more impurity or more mixing of classes.\n",
        "\n",
        " - Entropy:\n",
        "Entropy measures the amount of randomness or disorder in the node.\n",
        "Formula: Entropy = - Σ (pi log2 pi).\n",
        "Entropy is 0 when the node is pure. Higher entropy means the node has more uncertainty because it contains a mix of classes.\n",
        "\n",
        " - How they affect the split:\n",
        "A decision tree tries to split the data in such a way that the impurity after the split becomes as low as possible.\n",
        "  For Entropy, the tree calculates “Information Gain,” which is the reduction in entropy after the split. The split with the highest information gain is chosen. For Gini, the tree calculates the decrease in Gini Impurity after the split and chooses the split that reduces impurity the most."
      ],
      "metadata": {
        "id": "TbGlMMTUWgxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "  - Pre-Pruning and Post-Pruning are two techniques used to stop a decision tree from becoming too large and overfitting the data.\n",
        "\n",
        "- Pre-Pruning:\n",
        "Pre-pruning stops the growth of the tree early, before it becomes too deep.\n",
        "This is done by setting conditions like minimum samples required to split, maximum depth, or minimum information gain.\n",
        "If the split does not improve the model enough, the tree stops growing at that point.\n",
        "\n",
        "- Practical advantage of Pre-Pruning:\n",
        "It saves time and computation because the tree does not grow unnecessarily large.\n",
        "\n",
        "- Post-Pruning:\n",
        "Post-pruning allows the tree to grow fully first. After that, branches that do not improve performance are removed or replaced.\n",
        "This is usually done by checking accuracy on a validation set and cutting back the parts of the tree that cause overfitting.\n",
        "\n",
        "- Practical advantage of Post-Pruning:\n",
        "It usually gives better accuracy because the model first learns all patterns and then removes the noisy or less useful parts."
      ],
      "metadata": {
        "id": "OHkv_7R2ZDS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. : What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "   - nformation Gain is a measure used in decision trees to decide which feature gives the best split. It tells us how much purity (or reduction in impurity) we achieve after splitting a node using a particular feature.\n",
        "\n",
        "- Information Gain is calculated as:\n",
        "Information Gain = impurity of parent node - impurity of child nodes after the split.\n",
        "\n",
        "- A higher Information Gain means the feature helps separate the classes better and makes the child nodes purer.\n",
        "\n",
        "- It is important because the decision tree always chooses the split with the highest Information Gain, as this leads to better separation of the data, less impurity, and a more accurate model."
      ],
      "metadata": {
        "id": "P-EQWx5UZWXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "  - Decision trees are used in many real-world applications because they are simple to understand and interpret.\n",
        "\n",
        "- Common real-world applications:\n",
        "\n",
        "    1.Medical diagnosis - predicting diseases based on symptoms.\n",
        "\n",
        "    2.Banking and finance - loan approval, credit risk assessment, fraud detection.\n",
        "\n",
        "    3.Marketing - customer segmentation, predicting whether a customer will buy a product.\n",
        "\n",
        "    4.Agriculture - predicting crop yield, disease detection, and weather-based decisions.\n",
        "\n",
        "    5.Manufacturing - quality control and identifying defective products.\n",
        "\n",
        "- Main advantages:\n",
        "    1.Easy to understand and interpret, even by non-technical people.\n",
        "\n",
        "    2.Can handle both numerical and categorical data.\n",
        "\n",
        "    3.Requires little data preprocessing (no need for scaling or normalization).\n",
        "\n",
        "    4.Works well for non-linear relationships\n",
        "\n",
        "\n",
        "\n",
        "- Main limitations:\n",
        "\n",
        "    1.Prone to overfitting if not pruned.\n",
        "\n",
        "    2.Small changes in data can create a completely different tree (unstable).\n",
        "\n",
        "    3.Not as accurate as more advanced models like Random Forests or Gradient Boosting.\n",
        "\n",
        "    4.Can become very large and complex if not controlled."
      ],
      "metadata": {
        "id": "GDpYvUo-Zts2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.  Write a Python program to: ● Load the Iris Dataset ● Train a Decision Tree Classifier using the Gini criterion ● Print the model’s accuracy and feature importances .\n",
        "  -"
      ],
      "metadata": {
        "id": "qOZ6obfXaoER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "x= iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
        "\n",
        "\n",
        "model = DecisionTreeClassifier(criterion=\"gini\", random_state=1)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zorAZ9gmbLLk",
        "outputId": "36593bdd-84be-4956-aa02-50c3737e0ae5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9555555555555556\n",
            "Feature Importances: [0.02146947 0.02146947 0.06316954 0.89389153]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Write a Python program to: ● Load the Iris Dataset ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
      ],
      "metadata": {
        "id": "bTo_LQMgdX4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
        "\n",
        "\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
        "pruned_tree.fit(x_train, y_train)\n",
        "pruned_pred = pruned_tree.predict(x_test)\n",
        "pruned_accuracy = accuracy_score(y_test, pruned_pred)\n",
        "\n",
        "full_tree = DecisionTreeClassifier(random_state=1)\n",
        "full_tree.fit(x_train, y_train)\n",
        "full_pred = full_tree.predict(x_test)\n",
        "full_accuracy = accuracy_score(y_test, full_pred)\n",
        "\n",
        "print(\"Accuracy of Decision Tree (max_depth=3):\", pruned_accuracy)\n",
        "print(\"Accuracy of Fully-Grown Decision Tree:\", full_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpBtbO9idXNU",
        "outputId": "6c1bf622-f5c1-4722-c993-57e110268375"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree (max_depth=3): 0.9666666666666667\n",
            "Accuracy of Fully-Grown Decision Tree: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.: Write a Python program to: ● Load the Boston Housing Dataset ● Train a Decision Tree Regressor ● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "TMOjsyaYeCdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "x = boston.data\n",
        "y = boston.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
        "\n",
        "model = DecisionTreeRegressor(random_state=1)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n_OshpOeHZv",
        "outputId": "ee9eb078-c8a0-4df5-9731-76873c97f5d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 12.61375\n",
            "Feature Importances: [0.01867957 0.00071628 0.00141336 0.00230839 0.03006354 0.25015592\n",
            " 0.00676335 0.08579168 0.00326494 0.00857855 0.01020894 0.02886154\n",
            " 0.55319392]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.  Write a Python program to: ● Load the Iris Dataset ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV ● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "FqmKu70sfAb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "grid = GridSearchCV(dt, param_grid, cv=5)\n",
        "grid.fit(x_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E9goOuDfIcJ",
        "outputId": "0005d966-35ab-4496-8420-437aea8a5711"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to: ● Handle the missing values ● Encode the categorical features ● Train a Decision Tree model ● Tune its hyperparameters ● Evaluate its performance And describe what business value this model could provide in the real-world setting.\n",
        "   - Understand the Problem and Data: Before diving into the technical steps, it's crucial to thoroughly understand the problem (predicting a disease) and the nature of the dataset, including its size, mixed data types, and the extent of missing values. This step also involves defining the target variable and features.\n",
        "   \n",
        "   - Handle Missing Values: Explain various strategies for addressing missing data, such as imputation (mean, median, mode, or more advanced techniques like k-NN imputation) for numerical features, and mode imputation or creating a 'missing' category for categorical features. The choice depends on the amount of missing data and its potential impact.\n",
        "   - Encode Categorical Features: Describe methods for converting categorical features into a numerical format that a Decision Tree can process. This includes techniques like One-Hot Encoding for nominal variables and Label Encoding or Ordinal Encoding for ordinal variables, explaining when to use each.\n",
        "   - Train a Decision Tree Model: Detail the process of splitting the preprocessed dataset into training and testing sets. Then, explain how to initialize and train a Decision Tree Classifier (or Regressor, depending on the target variable) using a library like scikit-learn, briefly touching upon the Gini or Entropy criterion.\n",
        "  - Tune Model Hyperparameters: Explain the importance of hyperparameter tuning for Decision Trees to prevent overfitting and improve generalization. Discuss common hyperparameters like 'max_depth', 'min_samples_split', 'min_samples_leaf', and 'criterion'. Describe techniques like GridSearchCV or RandomizedSearchCV for systematically finding the optimal combination of these parameters.\n",
        "  - Evaluate Model Performance: Outline how to evaluate the performance of the tuned Decision Tree model on the unseen test set. Discuss relevant classification metrics such as accuracy, precision, recall, F1-score, and ROC AUC, emphasizing the importance of choosing metrics appropriate for the business problem (e.g., high recall for disease prediction to minimize false negatives).\n",
        "  - Describe Business Value: Articulate the real-world business value this predictive model could provide for the healthcare company. This includes aspects like early disease detection, improving patient outcomes, optimizing resource allocation, reducing healthcare costs, and enabling proactive interventions.\n",
        " - Final Task: Summarize the entire process and its implications."
      ],
      "metadata": {
        "id": "yhh-SiJyfmJF"
      }
    }
  ]
}